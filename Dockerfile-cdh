FROM centos:6.6

MAINTAINER diogo.melo@b2wdigital.com

RUN curl -o /etc/yum.repos.d/cloudera-cdh5.repo https://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/cloudera-cdh5.repo

# CDH 5.5.2
RUN sed -i 's/\/5\//\/5.5.2\//' /etc/yum.repos.d/cloudera-cdh5.repo

RUN yum update -y && yum install -y \
    hadoop-hdfs-namenode \
    hadoop-hdfs-datanode \
    # hadoop-yarn-resourcemanager \
    hive \
    hive-server2 \
    impala \
    impala-server \
    impala-state-store \
    impala-catalog \
    impala-shell \
    sudo \
    && yum clean all

# JAVA 8
RUN curl -LO 'http://download.oracle.com/otn-pub/java/jdk/8u102-b14/jdk-8u102-linux-x64.rpm' -H 'Cookie: oraclelicense=accept-securebackup-cookie'
RUN rpm -i jdk-8u102-linux-x64.rpm
RUN rm jdk-8u102-linux-x64.rpm

ENV JAVA_HOME /usr/java/default
ENV PATH $PATH:$JAVA_HOME/bin

# HADOOP Env's
ENV HADOOP_HOME=/usr/lib/hadoop/ \
    HADOOP_PREFIX=/usr/lib/hadoop/ \
    HADOOP_CONF_DIR=/etc/hadoop/conf/

COPY configs/hadoop/core-site.xml configs/hadoop/hdfs-site.xml $HADOOP_CONF_DIR

RUN sed -i s/HOSTNAME/0.0.0.0/g $HADOOP_CONF_DIR/*-site.xml

RUN mkdir -p /data/nn/ /data/dn/ && chown hdfs:hadoop /data/*

RUN mkdir -p /var/run/hdfs-sockets/ && chown hdfs:hadoop /var/run/hdfs-sockets/

# Hive Env's
ENV HIVE_HOME=/usr/lib/hive/ \
    HIVE_CONF_DIR=/etc/hive/conf/

### HIVE ###
COPY configs/hive/hive-site.xml /etc/hive/conf/

# Impala Env's
ENV IMPALA_HOME=/usr/lib/impala/ \
    IMPALA_CONF_DIR=/etc/impala/conf/

### IMPALA ###
COPY configs/hive/hive-site.xml /etc/impala/conf/

RUN curl -LO http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.39.tar.gz

RUN tar -zxf mysql-connector-* \
    && cp mysql-connector-*/mysql-connector* $HIVE_HOME/lib \
    && cp mysql-connector-*/mysql-connector* /var/lib/impala/ \
    && rm -rf mysql-connector-*

# Sqoop Env's
ENV SQOOP_HOME /usr/lib/sqoop
ENV PATH $PATH:$SQOOP_HOME/bin
ENV HBASE_HOME /root
ENV HCAT_HOME /root
ENV ACCUMULO_HOME /root

### SQOOP 1.4.6 ###
RUN curl -LO http://ftp.unicamp.br/pub/apache/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz

RUN tar -zxf sqoop-*.tar.gz \
    && mkdir -p $SQOOP_HOME \
    && mv sqoop-*/* $SQOOP_HOME \
    && rm -rf sqoop-*

RUN cp $HIVE_HOME/lib/mysql-* $SQOOP_HOME/lib

### SPARK 1.6.1 ###
RUN curl -LO http://ftp.unicamp.br/pub/apache/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz

ENV SPARK_HOME /usr/lib/spark
ENV PATH $PATH:/usr/lib/spark/bin

RUN tar -zxf spark-*.tgz \
    && mkdir -p $SPARK_HOME \
    && mv spark-*/* $SPARK_HOME \
    && rm -rf spark-* \
    && cd $SPARK_HOME \
    && mv conf/log4j.properties.template conf/log4j.properties \
    && sed -i 's/log4j.rootCategory=INFO/log4j.rootCategory=ERROR/' conf/log4j.properties

### PYTHON 2.7.10 ###
RUN yum groupinstall -y "Development tools" \
    && yum install -y zlib-devel \
    bzip2-devel \
    openssl-devel \
    ncurses-devel \
    sqlite-devel  \
    vim \
    && yum clean all

RUN curl -LO http://python.org/ftp/python/2.7.10/Python-2.7.10.tar.xz

RUN tar xf Python-*.tar.xz \
    && cd Python-* \
    && ./configure --prefix=/usr/local \
    && make && make altinstall \
    && rm -rf /Python*

RUN sed -i s/HOSTNAME/${HDFS_HOST}/g /etc/hadoop/conf/*-site.xml

RUN groupadd supergroup && usermod -a -G supergroup root

COPY bootstrap.sh /etc/bootstrap.sh

CMD ["/etc/bootstrap.sh", "-d"]